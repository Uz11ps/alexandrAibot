"""–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å AI (OpenAI)"""
import logging
import asyncio
import re
from typing import Optional, List, Dict
from openai import AsyncOpenAI
from openai import RateLimitError, APIError
import httpx
from config.settings import settings

logger = logging.getLogger(__name__)


def clean_ai_response(text: str) -> str:
    """
    –û—á–∏—â–∞–µ—Ç –æ—Ç–≤–µ—Ç AI –æ—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –∏ –ª–∏—à–Ω–µ–≥–æ —Ç–µ–∫—Å—Ç–∞
    """
    # –£–¥–∞–ª—è–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ AI –≤ –∫–æ–Ω—Ü–µ —Ç–µ–∫—Å—Ç–∞ (–Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è —Å "---" –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ "–≠—Ç–æ—Ç —Ç–µ–∫—Å—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç")
    lines = text.split('\n')
    cleaned_lines = []
    skip_rest = False
    
    for line in lines:
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º–∏ AI
        if line.strip().startswith('---'):
            skip_rest = True
            break
        if '–≠—Ç–æ—Ç —Ç–µ–∫—Å—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º' in line or '—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º –∫ –¥–ª–∏–Ω–µ' in line:
            skip_rest = True
            break
        if skip_rest:
            continue
        cleaned_lines.append(line)
    
    cleaned_text = '\n'.join(cleaned_lines).strip()
    
    # –£–¥–∞–ª—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –≤ –∫–æ–Ω—Ü–µ
    patterns_to_remove = [
        r'---.*$',
        r'–≠—Ç–æ—Ç —Ç–µ–∫—Å—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç.*$',
        r'—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º.*$',
        r'–¥–µ–ª–∞—è –µ–≥–æ –≤–∏–∑—É–∞–ª—å–Ω–æ.*$',
        r'–ª–µ–≥–∫–∏–º –¥–ª—è –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è.*$'
    ]
    
    for pattern in patterns_to_remove:
        cleaned_text = re.sub(pattern, '', cleaned_text, flags=re.MULTILINE | re.IGNORECASE)
    
    return cleaned_text.strip()


def markdown_to_html(text: str) -> str:
    """
    –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç markdown —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ HTML –¥–ª—è Telegram
    """
    # –ó–∞–º–µ–Ω—è–µ–º **—Ç–µ–∫—Å—Ç** –Ω–∞ <b>—Ç–µ–∫—Å—Ç</b>
    text = re.sub(r'\*\*(.+?)\*\*', r'<b>\1</b>', text)
    
    # –ó–∞–º–µ–Ω—è–µ–º *—Ç–µ–∫—Å—Ç* –Ω–∞ <i>—Ç–µ–∫—Å—Ç</i> (–∫—É—Ä—Å–∏–≤, –Ω–æ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ –¥–≤–æ–π–Ω—ã–µ –∑–≤–µ–∑–¥–æ—á–∫–∏)
    text = re.sub(r'(?<!\*)\*([^*\n]+?)\*(?!\*)', r'<i>\1</i>', text)
    
    # –ó–∞–º–µ–Ω—è–µ–º `—Ç–µ–∫—Å—Ç` –Ω–∞ <code>—Ç–µ–∫—Å—Ç</code>
    text = re.sub(r'`([^`]+?)`', r'<code>\1</code>', text)
    
    return text


class AIService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å OpenAI API"""
    
    def __init__(self, prompt_config_service=None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è AI —Å–µ—Ä–≤–∏—Å–∞
        """
        self.prompt_config_service = prompt_config_service
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–æ–∫—Å–∏ –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
        http_client = None
        self.proxy_list = []
        self.current_proxy_index = 0
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ª–∏ –º–æ–¥–µ–ª—å –ø–∞—Ä–∞–º–µ—Ç—Ä temperature
        self.supports_temperature = not (settings.OPENAI_MODEL.startswith("gpt-5") or 
                                         settings.OPENAI_MODEL.startswith("o1") or
                                         "o1" in settings.OPENAI_MODEL.lower())
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–ø–∏—Å–∫–∞ API –∫–ª—é—á–µ–π –¥–ª—è —Ä–æ—Ç–∞—Ü–∏–∏
        self.api_keys = [settings.OPENAI_API_KEY]
        if settings.OPENAI_API_KEYS:
            additional_keys = [k.strip() for k in settings.OPENAI_API_KEYS.split(',')]
            self.api_keys.extend(additional_keys)
        self.current_api_key_index = 0
        
        logger.info(f"–î–æ—Å—Ç—É–ø–Ω–æ API –∫–ª—é—á–µ–π: {len(self.api_keys)}")
        
        if settings.OPENAI_PROXY_ENABLED and settings.OPENAI_PROXY_URL:
            proxy_urls = [p.strip() for p in settings.OPENAI_PROXY_URL.split(',')]
            normalized_proxies = []
            for proxy in proxy_urls:
                if proxy.count(':') == 3 and not proxy.startswith('http'):
                    parts = proxy.split(':')
                    if len(parts) == 4:
                        domain, port, username, password = parts
                        proxy = f"http://{username}:{password}@{domain}:{port}"
                normalized_proxies.append(proxy)
            self.proxy_list = normalized_proxies
            
            proxy_url = normalized_proxies[0]
            http_client = httpx.AsyncClient(
                proxy=proxy_url,
                timeout=httpx.Timeout(300.0, connect=60.0, read=300.0)
            )
        
        self.client = AsyncOpenAI(
            api_key=self.api_keys[0],
            http_client=http_client
        )
        self.model = settings.OPENAI_MODEL
        self.proxy_enabled = settings.OPENAI_PROXY_ENABLED
        self.proxy_url = settings.OPENAI_PROXY_URL
    
    def _switch_proxy(self):
        """–ü–µ—Ä–µ–∫–ª—é—á–∞–µ—Ç—Å—è –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –ø—Ä–æ–∫—Å–∏ –∏–∑ —Å–ø–∏—Å–∫–∞"""
        if len(self.proxy_list) > 1:
            self.current_proxy_index = (self.current_proxy_index + 1) % len(self.proxy_list)
            new_proxy = self.proxy_list[self.current_proxy_index]
            http_client = httpx.AsyncClient(
                proxy=new_proxy,
                timeout=httpx.Timeout(300.0, connect=60.0, read=300.0)
            )
            self.client = AsyncOpenAI(
                api_key=self.api_keys[self.current_api_key_index],
                http_client=http_client
            )
            return True
        return False
    
    def _switch_api_key(self):
        """–ü–µ—Ä–µ–∫–ª—é—á–∞–µ—Ç—Å—è –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π API –∫–ª—é—á –∏–∑ —Å–ø–∏—Å–∫–∞"""
        if len(self.api_keys) > 1:
            self.current_api_key_index = (self.current_api_key_index + 1) % len(self.api_keys)
            new_key = self.api_keys[self.current_api_key_index]
            
            http_client = None
            if self.proxy_enabled and self.proxy_list:
                current_proxy = self.proxy_list[self.current_proxy_index]
                http_client = httpx.AsyncClient(
                    proxy=current_proxy,
                    timeout=httpx.Timeout(300.0, connect=60.0, read=300.0)
                )
            
            self.client = AsyncOpenAI(
                api_key=new_key,
                http_client=http_client
            )
            return True
        return False
    
    async def make_news_standalone(self, text: str) -> str:
        """
        –ü–µ—Ä–µ—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç—å –≤ –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω—ã–π –ø–æ—Å—Ç
        """
        if self.prompt_config_service:
            system_prompt = self.prompt_config_service.get_prompt("standalone_news", "system_prompt")
        else:
            system_prompt = "–°–¥–µ–ª–∞–π —Ç–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏ —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω—ã–º, —É–¥–∞–ª–∏–≤ –æ—Ç—Å—ã–ª–∫–∏ –∫ –ø—Ä–æ—à–ª—ã–º –ø–æ—Å—Ç–∞–º."
            
        try:
            request_params = {
                "model": self.model,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"–ü–µ—Ä–µ—Ä–∞–±–æ—Ç–∞–π —ç—Ç–æ—Ç —Ç–µ–∫—Å—Ç:\n\n{text}"}
                ],
                "max_completion_tokens": 2000
            }
            if self.supports_temperature:
                request_params["temperature"] = 0.5
            response = await self.client.chat.completions.create(**request_params)
            result = response.choices[0].message.content.strip()
            return markdown_to_html(clean_ai_response(result))
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ make_news_standalone: {e}")
            return text

    async def generate_post_text(
        self,
        prompt: str,
        context: Optional[str] = None,
        photos_description: Optional[str] = None
    ) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –ø–æ—Å—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–º–ø—Ç–∞ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        """
        if self.prompt_config_service:
            system_prompt = self.prompt_config_service.get_prompt("generate_post", "system_prompt")
            if not system_prompt:
                system_prompt = self._get_default_system_prompt()
        else:
            system_prompt = self._get_default_system_prompt()
        
        if photos_description:
            system_prompt += "\n\n**–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û:** –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –æ–ø–∏—Å–∞–Ω–∏–µ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π –Ω–∏–∂–µ. –ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –Ω–∏—á–µ–≥–æ —Å–≤–æ–µ–≥–æ."
        
        user_prompt = prompt
        if context:
            user_prompt += f"\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context}"
        if photos_description:
            user_prompt += f"\n\n–û–ø–∏—Å–∞–Ω–∏–µ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π:\n{photos_description}"
        
        try:
            timeout_seconds = 180.0 if self.proxy_enabled else 60.0
            
            if self.model.startswith("gpt-5") or "o1" in self.model.lower():
                messages = [{"role": "user", "content": f"–ò–ù–°–¢–†–£–ö–¶–ò–Ø:\n{system_prompt}\n\n–ó–ê–î–ê–ù–ò–ï:\n{user_prompt}"}]
            else:
                messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
            
            request_params = {
                "model": self.model,
                "messages": messages,
                "max_completion_tokens": 10000
            }
            if self.supports_temperature:
                request_params["temperature"] = 0.7
            
            response = await asyncio.wait_for(
                self.client.chat.completions.create(**request_params),
                timeout=timeout_seconds
            )
            
            if not response.choices or not response.choices[0].message.content:
                raise Exception("–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç AI")
            
            result = response.choices[0].message.content.strip()
            return markdown_to_html(clean_ai_response(result))
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞: {e}")
            return "üìä <b>–û—Ç—á–µ—Ç –∫–æ–º–ø–∞–Ω–∏–∏ ¬´–ê—Ä—Ö–µ–æ–Ω¬ª</b>\n\n–í –¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç –º—ã —Ä–∞–±–æ—Ç–∞–µ–º –Ω–∞–¥ –≤–∞—à–∏–º –æ–±—ä–µ–∫—Ç–æ–º. –ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ –±—É–¥—É—Ç –ø–æ–∑–∂–µ."

    async def analyze_photo(self, photo_path: str) -> str:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—é
        """
        import base64
        from pathlib import Path
        from PIL import Image
        import io
        
        try:
            with Image.open(photo_path) as img:
                if img.mode != 'RGB':
                    img = img.convert('RGB')
                max_size = 1024
                if max(img.size) > max_size:
                    img.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)
                buffer = io.BytesIO()
                img.save(buffer, format='JPEG', quality=85, optimize=True)
                image_data = buffer.getvalue()
        except Exception as e:
            with open(photo_path, "rb") as photo_file:
                image_data = photo_file.read()
        
        base64_image = base64.b64encode(image_data).decode('utf-8')
        
        analysis_prompt = self._get_photo_analysis_prompt()
        vision_model = "gpt-5.2"
        
        if vision_model.startswith("gpt-5") or "o1" in vision_model.lower():
            messages = [{
                "role": "user",
                "content": [
                    {"type": "text", "text": f"–ò–ù–°–¢–†–£–ö–¶–ò–Ø: –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–æ —Ñ–æ—Ç–æ –∫–∞–∫ —Ç–µ—Ö–Ω–∞–¥–∑–æ—Ä –ê—Ä—Ö–µ–æ–Ω.\n\n–ó–ê–î–ê–ù–ò–ï: {analysis_prompt}"},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}}
                ]
            }]
        else:
            messages = [{
                "role": "user",
                "content": [
                    {"type": "text", "text": analysis_prompt},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}}
                ]
            }]

        try:
            response = await asyncio.wait_for(
                self.client.chat.completions.create(
                    model=vision_model,
                    messages=messages,
                    max_completion_tokens=5000
                ),
                timeout=180.0 if self.proxy_enabled else 60.0
            )
            return response.choices[0].message.content.strip() or "–§–æ—Ç–æ —Å–æ —Å—Ç—Ä–æ–π–ø–ª–æ—â–∞–¥–∫–∏."
        except Exception:
            return "–§–æ—Ç–æ —Å–æ —Å—Ç—Ä–æ–π–ø–ª–æ—â–∞–¥–∫–∏ –ê—Ä—Ö–µ–æ–Ω."

    async def analyze_multiple_photos(self, photo_paths: List[str]) -> str:
        if not photo_paths: return ""
        descriptions = []
        for i, path in enumerate(photo_paths, 1):
            desc = await self.analyze_photo(path)
            descriptions.append(f"–§–æ—Ç–æ {i}: {desc}")
        return "\n\n".join(descriptions)

    async def analyze_video(self, video_path: str, frames_count: int = 8) -> str:
        try:
            import cv2
            import tempfile
            from pathlib import Path
            cap = cv2.VideoCapture(video_path)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            step = total_frames // (frames_count + 1)
            frame_indices = [step * (i + 1) for i in range(frames_count)]
            
            descriptions = []
            temp_dir = Path(tempfile.gettempdir()) / "video_frames"
            temp_dir.mkdir(exist_ok=True)
            
            for i, idx in enumerate(frame_indices):
                cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
                ret, frame = cap.read()
                if not ret: continue
                f_path = temp_dir / f"f_{i}.jpg"
                cv2.imwrite(str(f_path), frame)
                desc = await self.analyze_photo(str(f_path))
                descriptions.append(f"–°—Ü–µ–Ω–∞ {i+1}: {desc}")
                f_path.unlink(missing_ok=True)
            cap.release()
            return "\n\n".join(descriptions) or "–í–∏–¥–µ–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞."
        except Exception:
            return "–í–∏–¥–µ–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞ –ê—Ä—Ö–µ–æ–Ω."

    def _get_default_system_prompt(self) -> str:
        return """–¢—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∫–æ–ø–∏—Ä–∞–π—Ç–µ—Ä –∫–æ–º–ø–∞–Ω–∏–∏ "–ê—Ä—Ö–µ–æ–Ω". –ü–∏—à–∏ —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç–æ, —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ –∏ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ. –î–ª–∏–Ω–∞ 1500-2000 —Å–∏–º–≤–æ–ª–æ–≤."""

    def _get_photo_analysis_prompt(self) -> str:
        if self.prompt_config_service:
            return self.prompt_config_service.get_prompt("analyze_photo", "user_prompt") or "–û–ø–∏—à–∏ –¥–µ—Ç–∞–ª–∏ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞ –Ω–∞ —Ñ–æ—Ç–æ."
        return "–û–ø–∏—à–∏ –¥–µ—Ç–∞–ª–∏ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞ –Ω–∞ —Ñ–æ—Ç–æ."

    async def generate_post_from_sources(self, source_posts: List[Dict[str, str]]) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ—Å—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –ø–æ—Å—Ç–æ–≤ –∏–∑ –¥—Ä—É–≥–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        """
        if not source_posts:
            return "üèóÔ∏è –ù–æ–≤–æ—Å—Ç–∏ –ê—Ä—Ö–µ–æ–Ω: —Å–ª–µ–¥–∏—Ç–µ –∑–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è–º–∏."
        
        posts_text = []
        source_links = set()
        for i, post in enumerate(source_posts[:10], 1):
            text = post.get('text', '')
            link = post.get('source', '')
            if text: posts_text.append(f"–ò—Å—Ç–æ—á–Ω–∏–∫ {i}:\n{text}")
            if link: source_links.add(link)
        
        sources_context = "\n---\n".join(posts_text)
        links_str = "\n".join([f"‚Ä¢ {link}" for link in source_links])
        
        if self.prompt_config_service:
            system_prompt = self.prompt_config_service.get_prompt("generate_from_sources", "system_prompt")
        else:
            system_prompt = "–¢—ã –∞–Ω–∞–ª–∏—Ç–∏–∫ –ê—Ä—Ö–µ–æ–Ω. –ü–∏—à–∏ —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç–æ (1500-2000 —Å–∏–º–≤)."

        user_prompt = f"""–ù–∞–ø–∏—à–∏ —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π –ø–æ—Å—Ç (1500-2000 —Å–∏–º–≤–æ–ª–æ–≤) –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–∏—Ö –¥–∞–Ω–Ω—ã—Ö:\n{sources_context}
\n–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –í –∫–æ–Ω—Ü–µ –¥–æ–±–∞–≤—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ "üìå –ò—Å—Ç–æ—á–Ω–∏–∫–∏:" –∏ —Å–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫:\n{links_str}"""
        
        try:
            messages = [{"role": "user", "content": f"–ò–ù–°–¢–†–£–ö–¶–ò–Ø:\n{system_prompt}\n\n–ó–ê–î–ê–ù–ò–ï:\n{user_prompt}"}]
            response = await asyncio.wait_for(
                self.client.chat.completions.create(model=self.model, messages=messages, max_completion_tokens=4000),
                timeout=180.0
            )
            result = response.choices[0].message.content.strip()
            clean_text = clean_ai_response(result)
            
            # –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–û –ü–†–ò–ö–õ–ï–ò–í–ê–ï–ú –ò–°–¢–û–ß–ù–ò–ö–ò, –ï–°–õ–ò –ò–• –ù–ï–¢
            if source_links and "–ò—Å—Ç–æ—á–Ω–∏–∫–∏" not in clean_text:
                clean_text += f"\n\nüìå <b>–ò—Å—Ç–æ—á–Ω–∏–∫–∏:</b>\n{links_str}"
            
            return markdown_to_html(clean_text)
        except Exception:
            return f"üèóÔ∏è <b>–ù–æ–≤–æ—Å—Ç–∏ –ê—Ä—Ö–µ–æ–Ω</b>\n\n{sources_context[:500]}...\n\nüìå <b>–ò—Å—Ç–æ—á–Ω–∏–∫–∏:</b>\n{links_str}"

    def _get_fallback_source_post(self) -> str:
        return "üèóÔ∏è –ù–æ–≤–æ—Å—Ç–∏ –ê—Ä—Ö–µ–æ–Ω: —Å–ª–µ–¥–∏—Ç–µ –∑–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è–º–∏."

    async def analyze_sources(self, urls: List[str]) -> str:
        if not urls: return ""
        try:
            urls_text = "\n".join(urls)
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–∏ —Å—Å—ã–ª–∫–∏ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞:\n{urls_text}"}],
                max_completion_tokens=1000
            )
            return response.choices[0].message.content.strip()
        except Exception:
            return f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: {', '.join(urls)}"

    async def generate_meme_idea(self, topic: str) -> str:
        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": f"–ü—Ä–∏–¥—É–º–∞–π —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–π –º–µ–º: {topic}"}],
                max_completion_tokens=300
            )
            return response.choices[0].message.content.strip()
        except Exception:
            return "–ò–¥–µ—è –¥–ª—è –º–µ–º–∞: –ø—Ä–æ—Ä–∞–± –∏ —Å—Ä–æ–∫–∏."
